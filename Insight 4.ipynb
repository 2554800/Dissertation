{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "O6QFIr89dKPo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "327082b8-b012-44b8-f156-5fea1cebcf8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.7.9)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install -U datasets\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Review_dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_review_Cell_Phones_and_Accessories\", trust_remote_code=True)\n",
        "Meta_dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_meta_Cell_Phones_and_Accessories\", split=\"full\",trust_remote_code=True)"
      ],
      "metadata": {
        "id": "N55PVx7JtSIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter reviews where helpful_vote > 100\n",
        "helpful_reviews = [review for review in Review_dataset[\"full\"] if review.get(\"helpful_vote\", 0) > 1000]\n",
        "\n",
        "# Display the first few results (if any)\n",
        "print(helpful_reviews[:2])  # Prints first 5 reviews with helpful_vote > 50"
      ],
      "metadata": {
        "id": "hGVLS-BXt3vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Extract and convert relevant fields\n",
        "extracted_data = []\n",
        "for item in helpful_reviews:\n",
        "    extracted_data.append({\n",
        "        \"star_rating\": item.get(\"rating\"),\n",
        "        \"product_title\": item.get(\"title\"),\n",
        "        \"review_body\": item.get(\"text\"),\n",
        "        \"review_date\": datetime.utcfromtimestamp(item.get(\"timestamp\") / 1000).strftime('%Y-%m-%d'),\n",
        "        \"verified_purchase\": item.get(\"verified_purchase\"),\n",
        "        \"helpful_vote\": item.get(\"helpful_vote\"),\n",
        "        \"product_id\": item.get(\"parent_asin\"),\n",
        "        \"user_id\": item.get(\"user_id\"),\n",
        "    })"
      ],
      "metadata": {
        "id": "SfzdzWxgEuTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install textblob\n",
        "from textblob import TextBlob\n",
        "SA_Textblob=pd.DataFrame(extracted_data)\n",
        "# Apply TextBlob polarity\n",
        "SA_Textblob['sentiment_score'] = SA_Textblob['review_body'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "\n",
        "# Classify sentiment\n",
        "SA_Textblob['sentiment_label'] = SA_Textblob['sentiment_score'].apply(\n",
        "    lambda x: 'positive' if x > 0.05 else 'negative' if x < -0.05 else 'neutral'\n",
        ")\n",
        "\n",
        "SA_Textblob"
      ],
      "metadata": {
        "id": "6xIiG0UhFeca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Apply sentiment analysis\n",
        "SA_VADER = pd.DataFrame(extracted_data)\n",
        "SA_VADER['sentiment_score'] = SA_VADER['review_body'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
        "\n",
        "# Classify sentiment\n",
        "SA_VADER['sentiment_label'] = SA_VADER['sentiment_score'].apply(\n",
        "    lambda x: 'positive' if x > 0.05 else 'negative' if x < -0.05 else 'neutral'\n",
        ")\n",
        "\n",
        "SA_VADER.head()"
      ],
      "metadata": {
        "id": "0lUtM5wAGEFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SA_VADER"
      ],
      "metadata": {
        "id": "DNnH5ItQHAEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge all three on 'product id' and 'sentiment label'\n",
        "merged = SA_VADER.merge(\n",
        "    SA_Textblob[['product_id','user_id' ,'sentiment_label']],\n",
        "    on=['product_id','user_id' ,'sentiment_label'])\n",
        "merged=merged[((merged['star_rating'] > 3) & (merged['sentiment_label'] == 'positive')) |\n",
        "       (merged['star_rating'] < 3) & (merged['sentiment_label'] == 'negative')]\n",
        "merged.to_csv(\"df_review.csv\", index=True)\n",
        "merged"
      ],
      "metadata": {
        "id": "R0QPx1ADGl4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "vectorizer = CountVectorizer(strip_accents=ascii, stop_words='english', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',ngram_range=(2,2), analyzer='word', max_df=0.7, min_df=1, max_features=None, vocabulary=None)\n",
        "\n",
        "X = vectorizer.fit_transform(merged['review_body'])\n",
        "ngrams = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create frequency DataFrame\n",
        "ngram_freq = pd.DataFrame(X.toarray(), columns=ngrams).sum().sort_values(ascending=False)\n",
        "top_ngrams = ngram_freq.head(10)  # Get top 10 most frequent n-grams\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=top_ngrams.values, y=top_ngrams.index, palette='viridis')\n",
        "plt.title('Top 10 Most Frequent Bi-grams in Product Features')\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Bi-gram')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display results\n",
        "print(\"Top 10 Bi-grams:\")\n",
        "print(top_ngrams)"
      ],
      "metadata": {
        "id": "IjODviV_scN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# n- gram analysis\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def generate_N_grams(text,ngram=2):\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'<.*?>', '', text) # Remove HTML\n",
        "  text = re.sub(r'[^a-z\\s]', '', text) # Remove punctuation\n",
        "  words=[word for word in text.split(\" \") if word not in set(stopwords.words('english'))]\n",
        "  #print(\"Sentence after removing stopwords:\",words)\n",
        "  temp=zip(*[words[i:] for i in range(0,ngram)])\n",
        "  ans=[' '.join(ngram) for ngram in temp]\n",
        "  return ans"
      ],
      "metadata": {
        "id": "peElmtyMIXpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "positiveValues= defaultdict(int)\n",
        "#get the count of every word in both the columns of df_train and df_test dataframes where sentiment=\"positive\"\n",
        "for text in merged[merged.sentiment_label==\"positive\"].review_body:\n",
        "  for word in generate_N_grams(text):\n",
        "    positiveValues[word]+=1\n",
        "\n",
        "df_positive=pd.DataFrame(sorted(positiveValues.items(),key=lambda x:x[1],reverse=True))"
      ],
      "metadata": {
        "id": "-wXenUoTI6ZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negativeValues= defaultdict(int)\n",
        "\n",
        "#get the count of every word in both the columns of df_train and df_test dataframes where sentiment=\"negative\"\n",
        "for text in merged[merged.sentiment_label==\"negative\"].review_body:\n",
        "  for word in generate_N_grams(text):\n",
        "    negativeValues[word]+=1\n",
        "\n",
        "df_negative=pd.DataFrame(sorted(negativeValues.items(),key=lambda x:x[1],reverse=True))"
      ],
      "metadata": {
        "id": "hM69lt9NJTkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_negative)\n",
        "print(df_positive)"
      ],
      "metadata": {
        "id": "GTYjeUAeJS11"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}